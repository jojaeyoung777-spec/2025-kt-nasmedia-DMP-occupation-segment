"""
Elasticsearch Multi-search ê¸°ë°˜ ìœ„ì¹˜ ë§¤ì¹­ (ë™ê¸° + ë³‘ë ¬ ì²˜ë¦¬)

ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬:
- ë™ê¸° ì½”ë“œë¡œ ë‹¨ìˆœí™”
- ThreadPoolExecutorë¡œ ì—¬ëŸ¬ ë°°ì¹˜ ë™ì‹œ ì²˜ë¦¬
- I/O bound ì‘ì—…ì— ìµœì í™”
"""
import pandas as pd
import requests
import json
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from config.settings import ElasticsearchConfig, MatchingConfig, PathConfig
from core.elasticsearch import get_es_url


class SyncMatcher:
    """ë™ê¸° ë§¤ì¹­ ì—”ì§„ (ë³‘ë ¬ ì‹¤í–‰)"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.es_config = ElasticsearchConfig.ES_CONFIG
        self.index_name = ElasticsearchConfig.INDEX_NAME
        self.distance_config = ElasticsearchConfig.GEO_SEARCH_CONFIG
        self.batch_size = ElasticsearchConfig.BATCH_CONFIG['search_batch_size']
        self.concurrency = MatchingConfig.CONCURRENCY
        self.chunk_size = MatchingConfig.CHUNK_SIZE
        self.timeout = MatchingConfig.REQUEST_TIMEOUT
        self.max_retries = MatchingConfig.MAX_RETRIES

        # ES URL ë° ì¸ì¦
        self.es_url = get_es_url()
        self.auth = (self.es_config['user'], self.es_config['password'])

    def get_distance(self, place_type):
        """place_typeì— ë”°ë¥¸ ê²€ìƒ‰ ë°˜ê²½ ë°˜í™˜"""
        return self.distance_config.get(place_type, '300m')

    def sync_msearch(self, search_body):
        """
        ë™ê¸° msearch ìš”ì²­ (ì¬ì‹œë„ í¬í•¨)

        Args:
            search_body: msearch ìš”ì²­ ë³¸ë¬¸ (ë¦¬ìŠ¤íŠ¸)

        Returns:
            ì‘ë‹µ JSON ë˜ëŠ” None
        """
        url = f"{self.es_url}/_msearch"

        # msearchëŠ” newline-delimited JSON í˜•ì‹
        body = ""
        for item in search_body:
            body += json.dumps(item) + "\n"

        headers = {"Content-Type": "application/x-ndjson"}

        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    url,
                    data=body,
                    headers=headers,
                    auth=self.auth,
                    timeout=self.timeout,
                    verify=False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
                )

                if response.status_code == 200:
                    return response.json()
                else:
                    print(f"âš  ES ì˜¤ë¥˜ (status {response.status_code}): {response.text[:200]}")
                    if attempt < self.max_retries - 1:
                        continue
                    return None

            except Exception as e:
                print(f"âš  ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    continue
                return None

        return None

    def process_batch(self, batch_data):
        """
        ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ (ThreadPoolExecutorì—ì„œ ë³‘ë ¬ ì‹¤í–‰ë¨)

        Args:
            batch_data: (batch_idx, batch_locations, place_type) íŠœí”Œ

        Returns:
            ë§¤ì¹­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        batch_idx, batch, place_type = batch_data
        distance = self.get_distance(place_type)

        # msearch ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        search_body = []
        for user_lat, user_lon, adid in batch:
            search_body.append({"index": self.index_name})
            search_body.append({
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"place_type": place_type}},
                            {
                                "geo_distance": {
                                    "distance": distance,
                                    "location": {"lat": user_lat, "lon": user_lon}
                                }
                            }
                        ]
                    }
                },
                "sort": [{
                    "_geo_distance": {
                        "location": {"lat": user_lat, "lon": user_lon},
                        "order": "asc",
                        "unit": "m"
                    }
                }],
                "size": 1,
                "_source": True
            })

        # ë™ê¸° msearch ì‹¤í–‰
        response = self.sync_msearch(search_body)

        if not response or 'responses' not in response:
            return []

        # ì‘ë‹µ ì²˜ë¦¬
        batch_results = []
        for idx, resp in enumerate(response['responses']):
            if 'error' in resp or resp['hits']['total']['value'] == 0:
                continue

            user_lat, user_lon, adid = batch[idx]
            hit = resp['hits']['hits'][0]
            source = hit['_source']

            result = {
                'adid': adid,
                'lat': user_lat,
                'lon': user_lon,
                'distance': hit['sort'][0]
            }

            if place_type in ['high_school', 'university']:
                result.update({
                    'fac_cd': source.get('fac_cd'),
                    'ctp_cd': source.get('ctp_cd'),
                    'sig_cd': source.get('sig_cd'),
                    'emd_cd': source.get('emd_cd')
                })
            else:
                result.update({
                    'corp_cd': source.get('corp_cd'),
                    'corp_depth1_cd': source.get('corp_depth1_cd'),
                    'corp_depth2_cd': source.get('corp_depth2_cd'),
                    'corp_depth3_cd': source.get('corp_depth3_cd'),
                    'corp_depth4_cd': source.get('corp_depth4_cd'),
                    'corp_depth5_cd': source.get('corp_depth5_cd'),
                    'ctp_cd': source.get('ctp_cd'),
                    'sig_cd': source.get('sig_cd'),
                    'emd_cd': source.get('emd_cd')
                })

            batch_results.append(result)

        return batch_results

    def batch_find_nearest_places_parallel(self, place_type, user_locations):
        """
        ë³‘ë ¬ë¡œ ë°°ì¹˜ ë‹¨ìœ„ ê²€ìƒ‰ (ThreadPoolExecutor ì‚¬ìš©)

        Args:
            place_type: ì¥ì†Œ íƒ€ì…
            user_locations: ì‚¬ìš©ì ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸ [(lat, lon, adid), ...]

        Returns:
            ë§¤ì¹­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        # ë°°ì¹˜ë“¤ ìƒì„±
        batches = []
        for i in range(0, len(user_locations), self.batch_size):
            batch = user_locations[i:i+self.batch_size]
            batches.append((i, batch, place_type))

        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.concurrency) as executor:
            # ëª¨ë“  ë°°ì¹˜ ì‘ì—… ì œì¶œ
            futures = {executor.submit(self.process_batch, batch_data): batch_data
                      for batch_data in batches}

            # ì§„í–‰ë¥  í‘œì‹œí•˜ë©° ê²°ê³¼ ìˆ˜ì§‘
            with tqdm(total=len(batches), desc="  ë°°ì¹˜ ì²˜ë¦¬ (ë³‘ë ¬)") as pbar:
                for future in as_completed(futures):
                    batch_results = future.result()
                    results.extend(batch_results)
                    pbar.update(1)

        return results

    def match_locations(self, place_type, input_csv, output_csv):
        """
        ìœ„ì¹˜ ë§¤ì¹­ (ë™ê¸° + ë³‘ë ¬)

        Args:
            place_type: 'high_school', 'university', 'company'
            input_csv: ì…ë ¥ CSV ê²½ë¡œ
            output_csv: ì¶œë ¥ CSV ê²½ë¡œ
        """
        distance = self.get_distance(place_type)
        print(f"\n{'='*80}")
        print(f"ë§¤ì¹­ ì‹œì‘: {place_type} (ë™ê¸° + ë³‘ë ¬ ì²˜ë¦¬)")
        print(f"ì…ë ¥ íŒŒì¼: {input_csv}")
        print(f"ì¶œë ¥ íŒŒì¼: {output_csv}")
        print(f"ê²€ìƒ‰ ë°˜ê²½: {distance}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œì”©")
        print(f"ë³‘ë ¬ ì²˜ë¦¬: {self.concurrency}ê°œ ìŠ¤ë ˆë“œ ğŸš€")
        print(f"{'='*80}")

        if not Path(input_csv).exists():
            print(f"âœ— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_csv}")
            return False

        try:
            print(f"\nì‚¬ìš©ì ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

            all_results = []
            processed_count = 0
            matched_count = 0
            first_write = True  # ì²« ì €ì¥ ì—¬ë¶€ ì¶”ì 

            try:
                total_lines = sum(1 for _ in open(input_csv)) - 1
                print(f"ì „ì²´ ë°ì´í„°: {total_lines:,}í–‰")
            except:
                total_lines = None

            chunk_num = 0

            for user_chunk in pd.read_csv(input_csv, chunksize=self.chunk_size):
                chunk_num += 1

                # ì „ì²˜ë¦¬
                if place_type == 'company' and 'time_type' in user_chunk.columns:
                    before_count = len(user_chunk)
                    user_chunk = user_chunk[user_chunk['time_type'] == 'DAY']
                    if len(user_chunk) == 0:
                        continue
                    print(f"  ì²­í¬ {chunk_num}: time_type='DAY' í•„í„°ë§ ({before_count:,} â†’ {len(user_chunk):,}í–‰)")

                user_chunk = user_chunk.dropna(subset=['lat', 'lon'])
                if len(user_chunk) == 0:
                    continue

                print(f"\nì²­í¬ {chunk_num}: {len(user_chunk):,}í–‰ ì²˜ë¦¬ ì¤‘...")

                # ì‚¬ìš©ì ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸
                user_locations = [
                    (row['lat'], row['lon'], row['adid'])
                    for _, row in user_chunk.iterrows()
                ]

                # ë³‘ë ¬ ë°°ì¹˜ ê²€ìƒ‰
                chunk_results = self.batch_find_nearest_places_parallel(
                    place_type,
                    user_locations
                )

                all_results.extend(chunk_results)
                processed_count += len(user_locations)
                matched_count += len(chunk_results)

                print(f"  ë§¤ì¹­ ì™„ë£Œ: {len(chunk_results):,}/{len(user_locations):,}ê°œ (ëˆ„ì : {matched_count:,}/{processed_count:,})")

                # ì¤‘ê°„ ì €ì¥ (100,000ê°œë§ˆë‹¤)
                if len(all_results) >= MatchingConfig.INTERMEDIATE_SAVE_INTERVAL:
                    if first_write:
                        self._save_results(all_results, output_csv, mode='w', header=True)
                        first_write = False
                    else:
                        self._save_results(all_results, output_csv, mode='a', header=False)
                    all_results = []

            # ë‚¨ì€ ê²°ê³¼ ì €ì¥
            if all_results:
                if first_write:
                    self._save_results(all_results, output_csv, mode='w', header=True)
                else:
                    self._save_results(all_results, output_csv, mode='a', header=False)

            print(f"\n{'='*80}")
            print(f"ë§¤ì¹­ ì™„ë£Œ!")
            print(f"  - ì²˜ë¦¬: {processed_count:,}ê°œ")
            print(f"  - ë§¤ì¹­ ì„±ê³µ: {matched_count:,}ê°œ")
            print(f"  - ë§¤ì¹­ë¥ : {100*matched_count/processed_count:.1f}%")
            print(f"  - ì¶œë ¥ íŒŒì¼: {output_csv}")
            print(f"{'='*80}")

            return True

        except Exception as e:
            print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _save_results(self, results, output_csv, mode='w', header=True):
        """ê²°ê³¼ ì €ì¥"""
        if not results:
            return

        df = pd.DataFrame(results)
        Path(output_csv).parent.mkdir(parents=True, exist_ok=True)

        df.to_csv(output_csv, index=False, mode=mode, header=header, encoding='utf-8-sig')
        if mode == 'w' or header:
            print(f"  ê²°ê³¼ ì €ì¥: {output_csv} ({len(df):,}ê°œ)")
        else:
            print(f"  ì¤‘ê°„ ì €ì¥: {len(df):,}ê°œ ì¶”ê°€")

    def run_all_matching_jobs(self):
        """
        ëª¨ë“  ë§¤ì¹­ ì‘ì—… ì‹¤í–‰

        DMP ë°ì´í„°(_adid.csv)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤ì¹­
        """
        matching_jobs = [
            
            {
                'input': str(PathConfig.DMP_DATA_DIR / 'high_adid.csv'),
                'output': str(PathConfig.OUTPUT_DIR / 'high_school_matched.csv'),
                'type': 'high_school'
            },
            
            #{
            #    'input': str(PathConfig.DMP_DATA_DIR / 'univ_adid.csv'),
            #    'output': str(PathConfig.OUTPUT_DIR / 'university_matched.csv'),
            #    'type': 'university'
            #},
            
            #{
            #   'input': str(PathConfig.DMP_DATA_DIR / 'work_adid.csv'),
            #    'output': str(PathConfig.OUTPUT_DIR / 'company_matched.csv'),
            #    'type': 'company'
            #}
        ]

        for job in matching_jobs:
            self.match_locations(
                place_type=job['type'],
                input_csv=job['input'],
                output_csv=job['output']
            )
